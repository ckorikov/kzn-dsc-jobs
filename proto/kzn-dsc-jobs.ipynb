{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import operator\n",
    "import Stemmer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954\n",
      "954\n"
     ]
    }
   ],
   "source": [
    "f = open('kzn-dsc/dataset.json')\n",
    "jobs = json.load(f)\n",
    "print(len(jobs))\n",
    "jobs = [job for job in jobs if job['text'] != 'This message was deleted.']\n",
    "print(len(jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = defaultdict(int)\n",
    "\n",
    "for job in jobs:\n",
    "    for reaction in job['reactions']:\n",
    "        # print(reaction)\n",
    "        counter[reaction] += job['reactions'][reaction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_reactions = sorted(counter.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ico', 226),\n",
       " ('grammar', 234),\n",
       " ('venheads', 238),\n",
       " ('money_mouth_face', 249),\n",
       " ('heavy_plus_sign', 249),\n",
       " ('tinkoff', 269),\n",
       " ('rowing-galera', 279),\n",
       " ('+1::skin-tone-6', 308),\n",
       " ('nor', 340),\n",
       " ('mickey', 347),\n",
       " ('ramen', 374),\n",
       " ('hankey', 377),\n",
       " ('wat', 391),\n",
       " ('chains', 403),\n",
       " ('joy', 408),\n",
       " ('noexcel', 426),\n",
       " ('putin', 430),\n",
       " ('moneys', 442),\n",
       " ('facepalm', 553),\n",
       " ('+1::skin-tone-2', 622),\n",
       " ('moneybag', 663),\n",
       " ('sberbank', 770),\n",
       " ('eww', 881),\n",
       " ('fireball', 917),\n",
       " ('notbad', 1694),\n",
       " ('fire', 2089),\n",
       " ('galera', 3727),\n",
       " ('+1', 5590),\n",
       " ('ban', 6281),\n",
       " ('fork', 7084)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_reactions[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reactions = ['+1', 'fire', 'notbad', 'fireball', 'moneybag', 'heavy_plus_sign', 'money_mouth_face', '+1::skin-tone-2']\n",
    "bad_reactions = ['ban', 'fork', 'galera', 'eww', 'facepalm', 'wat', 'rowing-galera', 'are_you_fucking_kidding_me', 'noexcel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_good_reactions = sum([counter[reaction] for reaction in good_reactions])\n",
    "amount_of_bad_reactions = sum([counter[reaction] for reaction in bad_reactions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12073\n",
      "19831\n"
     ]
    }
   ],
   "source": [
    "print(amount_of_good_reactions)\n",
    "print(amount_of_bad_reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = Stemmer.Stemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer, RussianStemmer\n",
    "\n",
    "ru_stemmer = RussianStemmer()\n",
    "eng_stemmer = EnglishStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def perfotm_transformation(text):\n",
    "    text = text.lower()\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    text = text.translate(translator)\n",
    "    words_without_punctuation = text.split()\n",
    "    # 3. Stem words\n",
    "    stemmed_words = [eng_stemmer.stem(ru_stemmer.stem(word)) for word in words_without_punctuation]\n",
    "    \n",
    "    return ' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Привет! Мы в Dbrain (<https://dbrain.io>) снова (вот в прошлый раз: <https://opendatascience.slack.com/archives/C04DA5FUF/p1519145569000376>) ищем дата-саентистов, на этот раз нам нужно два человека: - синьор - джуниор плюс (знает основы, уверенно идет к миддлу) Требования: - Опыт работы с современным DL фреймворкам (pytorch, tf, keras, mxnet). Мы в основном предпочитаем pytorch, но это не принципиально; - Опыт работы с python (scipy stack); - Опыт работы с дженерик моделями sklearn, xgboost, etc; - Опыт работы в ОС Linux (GNU tools, bash); - Опыт запуска моделей в лайт прод, показ MVP. Будет плюсом:: - Знает SQL; - Знание C/C++. При наличии убедительных аргументов полная свобода в принятии ключевых решений в выборе подходов / алгоритмов / фреймворков. Ну и мы всегда готовы советовать и помогать со своей стороны. Перспективы: - Стать лидом отдельной команды, разрабатывающей конкретный проект; - Возможность реализовать свою амбициозную идею. Условия: - Красивый офис между Новокузнецкой и Павелецкой; - На кухне есть печенье, чай, кофемашина и все необходимое для сборки бутерброда; - Оформление по ТК; - Джуниору до 120к, синьору до 250к на руки. Резюме лучше всего присылать на <mailto:job@dbrain.io|job@dbrain.io> (в теме лучше всего указать свое имя и желаемую позицию), если будете писать в треде - могу что-то пропустить.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[4]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'привет мы в dbrain https dbrain i снов вот в прошл раз https opendatascienc slack com archiv c04da5fuf p1519145569000376 ищ дат саентист на этот раз нам нужн два человек синьор джуниор плюс знает основ уверен идет к миддл требован оп работ с современ dl фреймворк pytorch tf kera mxnet мы в основн предпочита pytorch но эт не принципиальн оп работ с python scip stack оп работ с дженерик модел sklearn xgboost etc оп работ в ос linux gnu tool bash оп запуск модел в лайт прод показ mvp будет плюс знает sql знан c c при налич убедительн аргумент полн свобод в принят ключев решен в выбор подход алгоритм фреймворк ну и мы всегд готов советова и помога со сво сторон перспектив стат лид отдельн команд разрабатыва конкретн проект возможн реализова сво амбициозн ид услов красив офис межд новокузнецк и павелецк на кухн ест печен ча кофемашин и все необходим для сборк бутерброд оформлен по тк джуниор до 120к синьор до 250к на рук резюм лучш всег присыла на mailt job dbrain i job dbrain i в тем лучш всег указа сво им и жела позиц есл будет писа в тред мог что то пропуст'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perfotm_transformation(jobs[4]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_texts = [perfotm_transformation(job['text']) for job in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_job_galera(job):\n",
    "    positive_reactions = sum([job['reactions'][reaction] for reaction in job['reactions'] if reaction in good_reactions])\n",
    "    negative_reactions = sum([job['reactions'][reaction] for reaction in job['reactions'] if reaction in bad_reactions])\n",
    "    return negative_reactions > positive_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = []\n",
    "for job in jobs:\n",
    "    if is_job_galera(job):\n",
    "        y_labels.append(1)\n",
    "    else:\n",
    "        y_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(transformed_texts)\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<954x14421 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 133577 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=19, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=1, random_state=19)\n",
    "logit.fit(X_tfidf, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8825995807127882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = logit.predict(X_tfidf)\n",
    "print(accuracy_score(predictions, y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "vectorizer_word = TfidfVectorizer(analyzer='word', stop_words='english', ngram_range=(1, 5))\n",
    "train_word = vectorizer_word.fit_transform(transformed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=19, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(C=1, random_state=19)\n",
    "logit.fit(train_word, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9769392033542977\n"
     ]
    }
   ],
   "source": [
    "predictions = logit.predict(train_word)\n",
    "print(accuracy_score(predictions, y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=15)\n",
    "clf = clf.fit(train_word, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8522012578616353\n"
     ]
    }
   ],
   "source": [
    "predictions = clf.predict(train_word)\n",
    "print(accuracy_score(predictions, y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(clf, out_file='small_tree_1.dot', filled=True, feature_names=vectorizer_word.get_feature_names())\n",
    "# для этого понадобится библиотека pydot (pip install pydot)\n",
    "!dot -Tpng 'small_tree.dot' -o 'small_tree_1.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Требуется Senior Data scientist в международную компанию. OSA Hybrid Platform – сервис на основе BIG DATA платформы, управляемый прикладным Искусственным Интеллектом, повышающий уровень наличия товара на полке. Сервис работает в режиме реального времени. В проекте участвуют торговые сети, входящие в ТОП 10 ритейлеров: Перекресток, Dixy, Auchan, METRO Cash &amp; Carry, АВ Daily и пр. Так же, задействованы крупнейшие компании производители, среди них: Pepsi, Coca-Cola, Danone, JTI, Mars, Loreal, SanInBev, Efes, Unilever, Эфко и пр. Обязанности: • Менторство DS команды проекта. • Расширение круга гипотез в процессе анализа поведения потребителей в офф-лайн ритейле (фуд). • Расширение круга факторов, доступных для анализа (meta data, open data). • Проектирование и реализация алгоритмов прогноза покупательского поведения основанные на больших данных, используя машинное обучение. • Дата инжиниринг. • Покупательская сегментация и целевой маркетинг. • Курирование работы по созданию Каталогов мастер данных, магазинов и покупателей (очистка, обогащение, присвоение атрибутов). Стек используемых технологий: • Python. • Machine learning. • Neural networks. • Deep learning. • Numpy. • xGboost. • Sklearn. • PostgreSQL. • ClickHouse. Офис: Москва м. Беговая, либо Киев мкр-н Воздвиженка. Мы предлагаем: • Достойную, справедливую зарплату по результатам собеседований. • Корпоративный оплачиваемый английский язык. • Посещение тематических конференций. • Работу по Аgile в самоуправляемой команде. • Гибкий график работы. • Мы предлагаем опыт работы разработки уникального продукта с применением искусственного интеллекта в сфере Ритейла и FMCG. Направляйте резюме на адрес: <mailto:y.kartashova@osahp.com|y.kartashova@osahp.com>', 'reactions': {'fork': 36, 'grammar': 9, 'ballpok': 1, 'pikachu_dancing': 3, 'ban': 20, 'baby_rage': 2, 'xgboost': 1}}\n",
      "1\n",
      "треб senior dat scientist в международн компан os hybrid platform – сервис на основ big dat платформ управля прикладн искусствен интеллект повыша уровен налич товар на полк сервис работа в режим реальн времен в проект участв торгов сет входя в топ 10 ритейлер перекресток dix aucha metr cash amp carr ав dail и пр так же задействова крупн компан производител сред них pep coc col danon jti mar lorea saninb efe unilev эфк и пр обязан • менторств ds команд проект • расширен круг гипотез в процесс анализ поведен потребител в офф лайн ритейл фуд • расширен круг фактор доступн для анализ met dat op dat • проектирован и реализац алгоритм прогноз покупательск поведен основа на больш дан использу машин обучен • дат инжиниринг • покупательск сегментац и целев маркетинг • курирован работ по создан каталог мастер дан магазин и покупател очистк обогащен присвоен атрибут стек используем технолог • python • machin learn • neura network • deep learn • nump • xgboost • sklearn • postgresql • clickhous офис москв м бегов либ ки мкр н воздвиженк мы предлага • достойн справедлив зарплат по результат собеседован • корпоративн оплачива английск язык • посещен тематическ конференц • работ по агил в самоуправля команд • гибк график работ • мы предлага оп работ разработк уникальн продукт с применен искусствен интеллект в сфер ритейл и fmcg направля резюм на адрес mailt y kartashov osahp com y kartashov osahp com\n"
     ]
    }
   ],
   "source": [
    "job_index = 850\n",
    "print(jobs[job_index])\n",
    "print(y_labels[job_index])\n",
    "print(transformed_texts[job_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9282178217821783\n",
      "0.7700205338809035\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(precision_score(predictions, y_labels))\n",
    "print(recall_score(predictions, y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict(model, X, y):\n",
    "    model.fit(X, y)\n",
    "    predictions = model.predict(X)\n",
    "    print(\"---------Training--------\")\n",
    "    print(\"Accuracy score: \" + str(accuracy_score(y, np.where(predictions > 0.5, 1, 0))))\n",
    "    print(\"Precision score: \" + str(precision_score(y, np.where(predictions > 0.5, 1, 0))))\n",
    "    print(\"Recall score: \" + str(recall_score(y, np.where(predictions > 0.5, 1, 0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Training--------\n",
      "Accuracy score: 0.7987421383647799\n",
      "Precision score: 0.6840277777777778\n",
      "Recall score: 0.9752475247524752\n"
     ]
    }
   ],
   "source": [
    "fit_predict(clf, X_tfidf, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6331236897274634"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(clf.predict(X_tfidf), y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Training--------\n",
      "Accuracy score: 0.9748427672955975\n",
      "Precision score: 1.0\n",
      "Recall score: 0.9392405063291139\n"
     ]
    }
   ],
   "source": [
    "fit_predict(logit, train_word, y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def run_cv(model, X, y, test):\n",
    "    n_folds = 10\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "    #skf = KFold(n_splits=10, random_state=0)\n",
    "    data_x = X\n",
    "    data_y = y\n",
    "    pred = []\n",
    "    models = []\n",
    "    tfidfs = []\n",
    "    for train_index, test_index in skf.split(data_x, data_y):\n",
    "        \n",
    "        model = lgb.LGBMRegressor(num_leaves=31,\n",
    "                        learning_rate=0.025,\n",
    "                        n_estimators=20)\n",
    "        \n",
    "        x_train, x_test = np.array(data_x)[train_index], np.array(data_x)[test_index]\n",
    "        y_train, y_test = np.array(data_y)[train_index], np.array(data_y)[test_index]\n",
    "        \n",
    "        tfidf = TfidfVectorizer(sublinear_tf=True, analyzer='word', stop_words='english', ngram_range=(1, 1))\n",
    "        #tfidf = vectorizer_word\n",
    "        tfidf.fit(x_train)\n",
    "        \n",
    "        x_train = tfidf.transform(x_train)\n",
    "        x_test = tfidf.transform(x_test)\n",
    "        test_tfidf = tfidf.transform(test)\n",
    "        \n",
    "#         char_vectorizer = TfidfVectorizer(\n",
    "#                             sublinear_tf=True,\n",
    "#                             analyzer='char',\n",
    "#                             stop_words='english',\n",
    "#                             ngram_range=(2, 6),\n",
    "#                             max_features=50000)\n",
    "#         char_vectorizer.fit(X)\n",
    "        \n",
    "#         train_char_features = char_vectorizer.transform(x_train)\n",
    "#         test_char_features = char_vectorizer.transform(x_test)\n",
    "#         tfidf = vectorizer_word\n",
    "#         x_train = tfidf.transform(x_train)\n",
    "#         x_test = tfidf.transform(x_test)\n",
    "#         x_train = hstack([train_char_features, x_train])\n",
    "#         x_test = hstack([test_char_features, x_test])\n",
    "        fit_predict(model, x_train, y_train)\n",
    "        y_pred = model.predict(x_test)\n",
    "        # score = accuracy_score(y_test, y_pred)\n",
    "        prediction = model.predict(test_tfidf)\n",
    "        pred.append(prediction[:])\n",
    "        print(sum(y_pred))\n",
    "        print(\"---------Validation------\")\n",
    "        print(\"Accuracy score: \" + str(accuracy_score(y_test, np.where(y_pred > 0.5, 1, 0))))\n",
    "        print(\"Precision score: \" + str(precision_score(y_test, np.where(y_pred > 0.5, 1, 0))))\n",
    "        print(\"Recall score: \" + str(recall_score(y_test, np.where(y_pred > 0.5, 1, 0))))\n",
    "        models.append(model)\n",
    "        tfidfs.append(tfidf)\n",
    "    return pred, models, tfidfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "train, test, y_train, y_test = train_test_split(transformed_texts, y_labels, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "gbm = lgb.LGBMRegressor(num_leaves=31,\n",
    "                        learning_rate=0.025,\n",
    "                        n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Training--------\n",
      "Accuracy score: 0.8493894165535957\n",
      "Precision score: 0.9952153110047847\n",
      "Recall score: 0.6540880503144654\n",
      "10.760597933680637\n",
      "---------Validation------\n",
      "Accuracy score: 0.6538461538461539\n",
      "Precision score: 1.0\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8439620081411127\n",
      "Precision score: 0.9765258215962441\n",
      "Recall score: 0.6540880503144654\n",
      "11.058846183582684\n",
      "---------Validation------\n",
      "Accuracy score: 0.6153846153846154\n",
      "Precision score: 1.0\n",
      "Recall score: 0.09090909090909091\n",
      "---------Training--------\n",
      "Accuracy score: 0.8466757123473542\n",
      "Precision score: 0.9812206572769953\n",
      "Recall score: 0.6572327044025157\n",
      "11.576782289081711\n",
      "---------Validation------\n",
      "Accuracy score: 0.5\n",
      "Precision score: 0.3333333333333333\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8263229308005428\n",
      "Precision score: 0.9896907216494846\n",
      "Recall score: 0.6037735849056604\n",
      "11.55725568051095\n",
      "---------Validation------\n",
      "Accuracy score: 0.6153846153846154\n",
      "Precision score: 0.6666666666666666\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8480325644504749\n",
      "Precision score: 0.9858490566037735\n",
      "Recall score: 0.6572327044025157\n",
      "11.693640365333472\n",
      "---------Validation------\n",
      "Accuracy score: 0.46153846153846156\n",
      "Precision score: 0.2\n",
      "Recall score: 0.09090909090909091\n",
      "---------Training--------\n",
      "Accuracy score: 0.8426051560379919\n",
      "Precision score: 1.0\n",
      "Recall score: 0.6352201257861635\n",
      "11.385276991577845\n",
      "---------Validation------\n",
      "Accuracy score: 0.5384615384615384\n",
      "Precision score: 0.4\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8344640434192673\n",
      "Precision score: 0.9622641509433962\n",
      "Recall score: 0.6415094339622641\n",
      "11.033555387263371\n",
      "---------Validation------\n",
      "Accuracy score: 0.46153846153846156\n",
      "Precision score: 0.0\n",
      "Recall score: 0.0\n",
      "---------Training--------\n",
      "Accuracy score: 0.8385345997286295\n",
      "Precision score: 0.9901477832512315\n",
      "Recall score: 0.6320754716981132\n",
      "11.678353012314924\n",
      "---------Validation------\n",
      "Accuracy score: 0.5\n",
      "Precision score: 0.3333333333333333\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8507462686567164\n",
      "Precision score: 0.9770642201834863\n",
      "Recall score: 0.6698113207547169\n",
      "10.83009368243271\n",
      "---------Validation------\n",
      "Accuracy score: 0.5384615384615384\n",
      "Precision score: 0.4\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8453188602442334\n",
      "Precision score: 0.9766355140186916\n",
      "Recall score: 0.6572327044025157\n",
      "11.400302490327231\n",
      "---------Validation------\n",
      "Accuracy score: 0.6153846153846154\n",
      "Precision score: 0.6\n",
      "Recall score: 0.2727272727272727\n",
      "---------Training--------\n",
      "Accuracy score: 0.8317503392130258\n",
      "Precision score: 0.9801980198019802\n",
      "Recall score: 0.6226415094339622\n",
      "11.497160860980022\n",
      "---------Validation------\n",
      "Accuracy score: 0.6923076923076923\n",
      "Precision score: 0.8\n",
      "Recall score: 0.36363636363636365\n",
      "---------Training--------\n",
      "Accuracy score: 0.8507462686567164\n",
      "Precision score: 0.985981308411215\n",
      "Recall score: 0.6635220125786163\n",
      "11.12288258126969\n",
      "---------Validation------\n",
      "Accuracy score: 0.5384615384615384\n",
      "Precision score: 0.3333333333333333\n",
      "Recall score: 0.09090909090909091\n",
      "---------Training--------\n",
      "Accuracy score: 0.8371777476255088\n",
      "Precision score: 0.9714285714285714\n",
      "Recall score: 0.6415094339622641\n",
      "10.833483647519904\n",
      "---------Validation------\n",
      "Accuracy score: 0.5769230769230769\n",
      "Precision score: 0.5\n",
      "Recall score: 0.09090909090909091\n",
      "---------Training--------\n",
      "Accuracy score: 0.8398914518317503\n",
      "Precision score: 0.9716981132075472\n",
      "Recall score: 0.6477987421383647\n",
      "11.63048383852683\n",
      "---------Validation------\n",
      "Accuracy score: 0.6153846153846154\n",
      "Precision score: 0.6666666666666666\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8414634146341463\n",
      "Precision score: 0.9902439024390244\n",
      "Recall score: 0.6383647798742138\n",
      "10.797886915562954\n",
      "---------Validation------\n",
      "Accuracy score: 0.64\n",
      "Precision score: 0.75\n",
      "Recall score: 0.2727272727272727\n",
      "---------Training--------\n",
      "Accuracy score: 0.8333333333333334\n",
      "Precision score: 0.9850746268656716\n",
      "Recall score: 0.6226415094339622\n",
      "11.204607988660507\n",
      "---------Validation------\n",
      "Accuracy score: 0.56\n",
      "Precision score: 0.5\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8306233062330624\n",
      "Precision score: 0.9617224880382775\n",
      "Recall score: 0.6320754716981132\n",
      "11.087588474893423\n",
      "---------Validation------\n",
      "Accuracy score: 0.6\n",
      "Precision score: 0.6\n",
      "Recall score: 0.2727272727272727\n",
      "---------Training--------\n",
      "Accuracy score: 0.8428184281842819\n",
      "Precision score: 0.9855769230769231\n",
      "Recall score: 0.6446540880503144\n",
      "11.130489748967605\n",
      "---------Validation------\n",
      "Accuracy score: 0.68\n",
      "Precision score: 0.8\n",
      "Recall score: 0.36363636363636365\n",
      "---------Training--------\n",
      "Accuracy score: 0.8523035230352304\n",
      "Precision score: 0.9815668202764977\n",
      "Recall score: 0.6698113207547169\n",
      "10.532784489137539\n",
      "---------Validation------\n",
      "Accuracy score: 0.52\n",
      "Precision score: 0.3333333333333333\n",
      "Recall score: 0.09090909090909091\n",
      "---------Training--------\n",
      "Accuracy score: 0.8482384823848238\n",
      "Precision score: 0.9813084112149533\n",
      "Recall score: 0.660377358490566\n",
      "10.775531582645636\n",
      "---------Validation------\n",
      "Accuracy score: 0.4\n",
      "Precision score: 0.0\n",
      "Recall score: 0.0\n",
      "---------Training--------\n",
      "Accuracy score: 0.8590785907859079\n",
      "Precision score: 0.9953703703703703\n",
      "Recall score: 0.6761006289308176\n",
      "10.353674945522917\n",
      "---------Validation------\n",
      "Accuracy score: 0.6\n",
      "Precision score: 1.0\n",
      "Recall score: 0.09090909090909091\n",
      "---------Training--------\n",
      "Accuracy score: 0.8387533875338753\n",
      "Precision score: 0.9853658536585366\n",
      "Recall score: 0.6352201257861635\n",
      "10.980464020919824\n",
      "---------Validation------\n",
      "Accuracy score: 0.68\n",
      "Precision score: 1.0\n",
      "Recall score: 0.2727272727272727\n",
      "---------Training--------\n",
      "Accuracy score: 0.8455284552845529\n",
      "Precision score: 0.9903846153846154\n",
      "Recall score: 0.6477987421383647\n",
      "10.818747176869884\n",
      "---------Validation------\n",
      "Accuracy score: 0.48\n",
      "Precision score: 0.3333333333333333\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8346883468834688\n",
      "Precision score: 0.9757281553398058\n",
      "Recall score: 0.6320754716981132\n",
      "10.61722171903087\n",
      "---------Validation------\n",
      "Accuracy score: 0.64\n",
      "Precision score: 0.75\n",
      "Recall score: 0.2727272727272727\n",
      "---------Training--------\n",
      "Accuracy score: 0.8482384823848238\n",
      "Precision score: 0.9858490566037735\n",
      "Recall score: 0.6572327044025157\n",
      "10.859849392661769\n",
      "---------Validation------\n",
      "Accuracy score: 0.56\n",
      "Precision score: 0.5\n",
      "Recall score: 0.2727272727272727\n",
      "---------Training--------\n",
      "Accuracy score: 0.8292682926829268\n",
      "Precision score: 0.9848484848484849\n",
      "Recall score: 0.6132075471698113\n",
      "10.300516142405101\n",
      "---------Validation------\n",
      "Accuracy score: 0.6\n",
      "Precision score: 1.0\n",
      "Recall score: 0.09090909090909091\n",
      "---------Training--------\n",
      "Accuracy score: 0.8509485094850948\n",
      "Precision score: 0.985981308411215\n",
      "Recall score: 0.6635220125786163\n",
      "10.819653395466577\n",
      "---------Validation------\n",
      "Accuracy score: 0.68\n",
      "Precision score: 0.8\n",
      "Recall score: 0.36363636363636365\n",
      "---------Training--------\n",
      "Accuracy score: 0.8550135501355014\n",
      "Precision score: 0.9817351598173516\n",
      "Recall score: 0.6761006289308176\n",
      "10.350402660744557\n",
      "---------Validation------\n",
      "Accuracy score: 0.56\n",
      "Precision score: 0.5\n",
      "Recall score: 0.09090909090909091\n",
      "---------Training--------\n",
      "Accuracy score: 0.8523035230352304\n",
      "Precision score: 0.986046511627907\n",
      "Recall score: 0.6666666666666666\n",
      "10.907744501752621\n",
      "---------Validation------\n",
      "Accuracy score: 0.56\n",
      "Precision score: 0.5\n",
      "Recall score: 0.18181818181818182\n",
      "---------Training--------\n",
      "Accuracy score: 0.8525033829499323\n",
      "Precision score: 0.981651376146789\n",
      "Recall score: 0.670846394984326\n",
      "9.858331607263828\n",
      "---------Validation------\n",
      "Accuracy score: 0.625\n",
      "Precision score: 1.0\n",
      "Recall score: 0.1\n",
      "---------Hold-out---------\n",
      "Accuracy score: 0.6335078534031413\n",
      "Precision score: 0.631578947368421\n",
      "Recall score: 0.16\n"
     ]
    }
   ],
   "source": [
    "y_pred = run_cv(gbm, train, y_train, test)\n",
    "\n",
    "y_pred = np.mean(y_pred, axis = 0)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"---------Hold-out---------\")\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  1,  1,  0,  0,  0,  0,  1,  0,  1,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  1,  1,  1,  0,  0,  0,  0,  0,  1,  0,  0,  1,\n",
       "        0,  0,  0,  0,  0,  0,  0,  1, -1,  0,  1,  0,  0,  0,  1,  1,  1,\n",
       "        1,  0,  0,  1,  0,  1,  1,  1,  0,  1,  1,  0,  0,  1,  0, -1,  0,\n",
       "        0,  1,  0,  1,  1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  1,  0,  0,\n",
       "        1,  0,  0,  1,  0,  0,  0,  1,  1, -1,  0,  0,  0,  0,  1,  0,  1,\n",
       "        0, -1,  1,  1,  0,  0,  1,  0,  1,  0,  0,  0,  0,  0,  0, -1,  1,\n",
       "       -1,  0,  1,  0,  1,  0,  0,  1,  1,  1,  0,  0,  0,  0,  1, -1,  0,\n",
       "       -1,  0, -1,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  1,\n",
       "        0,  1, -1,  0,  1,  1, -1, -1,  0, -1,  1,  0,  0,  1,  1,  1,  0,\n",
       "        0,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  1,  0,  0,\n",
       "       -1,  1,  0,  0])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'дел в том что формальн ваканс соответств требован оформлен описан компан ест хот и ироничн написа ваканс четк указа треб дополнительн человек в поездк студент стажер и продавец оплат указа есл вы дума что мне за не стыдн вы ошиба то что он довольн развязн язык написа я соглас эт как бы парод на всяк лев маркетингов агентств котор мы конечн сам и явля и понима эт но хот у нас маркетингов агентств никт не люб имен так компан врод наш дела больш и очен тяжел работ по привлечен иностра денег в отечествен команд я горж что к эт причаст да конкуренц огромн и огромн част проект не продвига дальш пресейл о чем честн и написа в ваканс задач датамонстр так выход на заказчик получа у них четк постановк задач по форм передава эт форм отечественнвм команд и получа от команд предложен для заказчик тож качествен и быстр оформлен вот эт взаимодейств межд иностра заказчик и отечествен команд част явля сам узк мест част техническ российск команд отличн подкова но из за тог что он например плох знает английск ил коряв и долг формулир предложен из за тог что ест барьер менталитет эт команд неконкурентоспособн на глобальн рынк и мы как раз так команд хот помоч сам мы тож дела проект у нас изначальн производствен а не маркетингов компан но больш одн двух проект физическ мы не мож внутрен команд дела вообщ я не перв человек в компан поэт не счита себ вправ от е имен говор но раз уж тут так карнава то поч бы нет мен обескураж уровен ненавист с котор встрет ваканс так видим у нас относ ко всем кто слишк выпендрива хот эт вовс не выпендреж а самоирон u040hkje7 вы конечн может удал этот пост но я сам за то чтоб он оста пот что тут мног интересн обсужден и лулз что называ жал есл эт все пропадет тем бол что я ег все так размест в надежд что кто то откликнет на ваканс и нескольк адекватн люд уж откликнул'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    formatted_text = perfotm_transformation(text)\n",
    "    prob = []\n",
    "    for model, tfidf in zip(models, tfidfs):\n",
    "        features = tfidf.transform([formatted_text])\n",
    "        prob.append(model.predict(features))\n",
    "        \n",
    "    return (np.mean(prob) > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Training--------\n",
      "Accuracy score: 0.8100233100233101\n",
      "Precision score: 0.9854368932038835\n",
      "Recall score: 0.559228650137741\n",
      "40.94015226262435\n",
      "---------Validation------\n",
      "Accuracy score: 0.5833333333333334\n",
      "Precision score: 0.5384615384615384\n",
      "Recall score: 0.17073170731707318\n",
      "---------Training--------\n",
      "Accuracy score: 0.8193473193473193\n",
      "Precision score: 0.9905660377358491\n",
      "Recall score: 0.5785123966942148\n",
      "40.50830636933579\n",
      "---------Validation------\n",
      "Accuracy score: 0.5625\n",
      "Precision score: 0.46153846153846156\n",
      "Recall score: 0.14634146341463414\n",
      "---------Training--------\n",
      "Accuracy score: 0.8146853146853147\n",
      "Precision score: 0.9722222222222222\n",
      "Recall score: 0.5785123966942148\n",
      "41.262836761423294\n",
      "---------Validation------\n",
      "Accuracy score: 0.6041666666666666\n",
      "Precision score: 0.6\n",
      "Recall score: 0.21951219512195122\n",
      "---------Training--------\n",
      "Accuracy score: 0.8216783216783217\n",
      "Precision score: 0.9952830188679245\n",
      "Recall score: 0.581267217630854\n",
      "40.958168342239425\n",
      "---------Validation------\n",
      "Accuracy score: 0.5729166666666666\n",
      "Precision score: 0.5\n",
      "Recall score: 0.14634146341463414\n",
      "---------Training--------\n",
      "Accuracy score: 0.8277066356228172\n",
      "Precision score: 0.9864864864864865\n",
      "Recall score: 0.6016483516483516\n",
      "39.667765445559745\n",
      "---------Validation------\n",
      "Accuracy score: 0.5789473684210527\n",
      "Precision score: 0.5\n",
      "Recall score: 0.125\n",
      "---------Training--------\n",
      "Accuracy score: 0.8137369033760187\n",
      "Precision score: 0.9811320754716981\n",
      "Recall score: 0.5714285714285714\n",
      "39.29703607712077\n",
      "---------Validation------\n",
      "Accuracy score: 0.6105263157894737\n",
      "Precision score: 0.7142857142857143\n",
      "Recall score: 0.125\n",
      "---------Training--------\n",
      "Accuracy score: 0.8137369033760187\n",
      "Precision score: 0.9636363636363636\n",
      "Recall score: 0.5824175824175825\n",
      "40.0942617867621\n",
      "---------Validation------\n",
      "Accuracy score: 0.5894736842105263\n",
      "Precision score: 0.5384615384615384\n",
      "Recall score: 0.175\n",
      "---------Training--------\n",
      "Accuracy score: 0.8090803259604191\n",
      "Precision score: 0.9854368932038835\n",
      "Recall score: 0.5576923076923077\n",
      "40.74487918167441\n",
      "---------Validation------\n",
      "Accuracy score: 0.5578947368421052\n",
      "Precision score: 0.42857142857142855\n",
      "Recall score: 0.15\n",
      "---------Training--------\n",
      "Accuracy score: 0.8218859138533178\n",
      "Precision score: 0.9688888888888889\n",
      "Recall score: 0.5989010989010989\n",
      "39.526977956075434\n",
      "---------Validation------\n",
      "Accuracy score: 0.631578947368421\n",
      "Precision score: 0.8571428571428571\n",
      "Recall score: 0.15\n",
      "---------Training--------\n",
      "Accuracy score: 0.8242142025611175\n",
      "Precision score: 0.9775784753363229\n",
      "Recall score: 0.5989010989010989\n",
      "40.71909696483184\n",
      "---------Validation------\n",
      "Accuracy score: 0.5368421052631579\n",
      "Precision score: 0.35714285714285715\n",
      "Recall score: 0.125\n",
      "---------Hold-out---------\n",
      "Accuracy score: 0.774869109947644\n",
      "Precision score: 0.9444444444444444\n",
      "Recall score: 0.4533333333333333\n"
     ]
    }
   ],
   "source": [
    "y_pred, models, tfidfs = run_cv(gbm, transformed_texts, y_labels, test)\n",
    "\n",
    "y_pred = np.mean(y_pred, axis = 0)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "print(\"---------Hold-out---------\")\n",
    "print(\"Accuracy score: \" + str(accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision score: \" + str(precision_score(y_test, y_pred)))\n",
    "print(\"Recall score: \" + str(recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0),\n",
       " LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.025, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=20, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "треб middl senior te lead dat engin в компан ciklum с локац ки ил львов на ваш усмотрен flag u обязан • выбор и интеграц фреймворк big dat необходим для решен бизнес задач клиент • возможн участ в пре сейл общен с клиент • реализац etl • разработк backend на питон • определен политик хранен дан требован • solid understand of distribut comput principl • manag of hadoop cluster with all includ servic and configur of associ hardwar and softwar • python knowledg might be scal • proficienc with hadoop or cassandr map reduc hdfs • experienc with build stre process system use solut such as storm or spark stream • good knowledg of big dat queri tool such as pig hiv and impa • experienc with spark • experienc with integr of dat from multipl dat sourc • experienc with nosql databas such as hbas • experienc with рдбмс'с lik oracl and mysql • knowledg of various etl techniqu and framework • experienc with various messag system such as kafk experienc with big dat ml toolkit such as mahout sparkml or h2o • good understand of lambd architectur along with it advantag and drawback • experienc with clouder mapr hortonwork would be an advantag для лид и синьйор добавля в требован • больш оп • управлен команд и реализац реальн больш проект с применен биг дат • будет ответствен за принят решен по реализац проект и выбор инструмент про персональн скилл и предложен можн взят из ваканс выш для дс зп для лид может быт выш формальн описан https job ciklum com job middl dat engin ciklum cto lviv https job ciklum com job senior dat engin ciklum cto offic ky https job ciklum com job lead dat engin ciklum cto offic ky\n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "index = 801\n",
    "print(transformed_texts[index])\n",
    "print(y_labels[index])\n",
    "print(process_text(jobs[index]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ребят, привет! заранее извиняюсь, если пишу не в нужном формате (старался по возможности придерживаться), один хороший знакомый просил найти людей на проекты, связанные с анализом данных (все задачи и клиенты, описанные ниже проверены мной на адекватность): - Рекомендательная система для сети фитнес-клубов (данных много, есть полное их описание, семплы - гавернанс в компании на высшем уровне) - Рекомендательная система для крупной авиакомпании (тут задача в первую очередь - общаться с заказчиком и свести задачу к формальному описанию/ТЗ) По первому проекту - уже есть четкое понимание самой задачи, pipeline, все механики и превентивные меры, есть семплы данных, а также примерные требуетмые метрики качества. По второму - требуется общаться с заказчиком (довольно продвинутым) для более детальной проработки кейса Требования к кандидатам (очень субьективные, поэтому все равно пишите): наличие завершенных и внедренных аналитических продуктов. Тут не требуется выжимать доли качества, а важна аккуратность работы с данными и последовательность. По деньгам - обсуждается от требований кандидата (либо ЗП, либо оплата за весь проект). Кому интересно, либо есть какие-либо уточняющие вопросы - пишите на <mailto:al.krot.kav@gmail.com|al.krot.kav@gmail.com>'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( models, open( \"models.p\", \"wb\" ) )\n",
    "pickle.dump( tfidfs, open( \"tfidfs.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo(bot, update):\n",
    "    \"\"\"Echo the user message.\"\"\"\n",
    "    # update.message.reply_text(update.message.text)\n",
    "    if process_text(update.message.text):\n",
    "        update.message.reply_text('👎')\n",
    "    else:\n",
    "        update.message.reply_text('👍')\n",
    "    # update.message.reply_text('👎👍')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Требуется *Middle/Senior/Team Lead Data Engineer* в компанию *Ciklum* с локацией *Киев* или *Львов* (на ваше усмотрение) :flag-ua: *Обязаности:* • выбор и интеграция фреймворков big data, необходимых для решения бизнес задач клиентов • возможное участие в пре-сейлах, общение с клиентами • реализация ETL • разработка backend на питоне • определении политики хранения данных *Требования:* • solid understanding of distributed computing principles • Management of Hadoop cluster, with all included services and configuration of associated hardware and software • Python knowledge, might be Scala • Proficiency with Hadoop or Cassandra, Map Reduce, HDFS • Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming • Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala • Experience with Spark • Experience with integration of data from multiple data sources • Experience with NoSQL databases, such as HBase • Experience with RDBMS’s like Oracle and MySQL • Knowledge of various ETL techniques and frameworks. • Experience with various messaging systems, such as Kafka- Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O • Good understanding of Lambda Architecture, along with its advantages and drawbacks • Experience with Cloudera/MapR/Hortonworks would be an advantage Для Лида и Синьйора добавляется в требованиях: • больший опыт • управление командой и реализация реальных больших проектов с применением биг даты • будете ответственны за принятия решений по реализации проектов и выбору инструментов. *Про персональные скиллы и предложения можно взять из вакансии выше для дс. (зп для лида может быть выше)* *Формальное описание:* <https://jobs.ciklum.com/jobs/middle-data-engineer-ciklum-cto-lviv/> , <https://jobs.ciklum.com/jobs/senior-data-engineer-ciklum-cto-office-kyiv/> , <https://jobs.ciklum.com/jobs/lead-data-engineer-ciklum-cto-office-kyiv/>',\n",
       " 'reactions': {'fork': 13,\n",
       "  'vradchenko': 7,\n",
       "  'flag-ua': 4,\n",
       "  '+1': 1,\n",
       "  'tada': 1,\n",
       "  'galera': 1}}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs[801]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e1243bd22c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
